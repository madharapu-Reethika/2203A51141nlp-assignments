{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBhx8FlivR8T+uxKl6Uc8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madharapu-Reethika/2203A51141nlp-assignments/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "KfhFTaJef8Si"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_data = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods...\"\"\"\n"
      ],
      "metadata": {
        "id": "YymUVkNxgIrS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, text, max_length=50):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        encodings = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "        self.input_ids.append(torch.tensor(encodings['input_ids']))\n",
        "        self.attn_masks.append(torch.tensor(encodings['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attn_masks[idx]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "YjWBovDfgL52"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMgXb29YgPC4",
        "outputId": "fdf3fd87-d7a4-4352-e42b-f86082e8bfd8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = TextDataset(tokenizer=tokenizer, text=text_data)\n"
      ],
      "metadata": {
        "id": "Uivl691ZgSFb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, inputs, return_outputs=False):\n",
        "    \"\"\"\n",
        "    Computes the loss for the GPT-2 model.\n",
        "\n",
        "    Args:\n",
        "        model: The GPT-2 model.\n",
        "        inputs: A dictionary containing the input tensors.\n",
        "        return_outputs: Whether to return the model outputs along with the loss.\n",
        "\n",
        "    Returns:\n",
        "        The loss value, or a tuple of (loss, outputs) if return_outputs is True.\n",
        "    \"\"\"\n",
        "    labels = inputs.get(\"input_ids\").clone()\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    return (outputs.loss, outputs) if return_outputs else outputs.loss\n"
      ],
      "metadata": {
        "id": "WePb9rrZgXUK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        return compute_loss(model, inputs, return_outputs)\n"
      ],
      "metadata": {
        "id": "eU-UFh1qgbno"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(epochs):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=1,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "id": "AdWj2oRugevM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epochs in [20, 60, 70]:\n",
        "    print(f\"\\nTraining model for {epochs} epochs...\")\n",
        "    train_model(epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "OLAJksHCgh0g",
        "outputId": "898d7756-7533-42b8-ff8a-282ca323a480"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model for 20 epochs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 01:37, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model for 60 epochs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:33, Epoch 60/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model for 70 epochs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [70/70 04:36, Epoch 70/70]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, max_length=50):\n",
        "    inputs = tokenizer.encode(seed_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "ZCouw9mjgy0P"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seed_text = \"Once upon a time\"\n",
        "generated_text = generate_text(seed_text)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luIk_5a7g0UD",
        "outputId": "bef0f8dd-3f41-4b10-9ac5-9ddfe7929db4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods... She loved to visit her grandmother, who lived in the woods... She loved to visit her grandmother, who lived\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna"
      ],
      "metadata": {
        "id": "2W-w_Zp5mjhJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    global inputs\n",
        "    seed_text = \"Once upon a time\"\n",
        "    inputs = tokenizer.encode(seed_text, return_tensors=\"pt\")\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-4)\n",
        "    epochs = trial.suggest_int('epochs', 10, 80)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        outputs = model(input_ids=inputs[\"input_ids\"], labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "UuAhuC68neBH"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}